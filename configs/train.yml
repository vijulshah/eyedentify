seed: 42

dataset_configs:
  data_path: "data/eyes/left_eyes"
  dataset_paths_registry: "pd_ds_paths"
  strategy: "cross_validation" # any from: ["cross_validation", "stratify_participants", "random"] Default: "random"
  left_out_participants_for_val: [3, 7, 15, 44, 51] # only for cross_validation strategy
  left_out_participants_for_test: [1, 4, 6, 25, 36] # only for cross_validation strategy
  dataset_paths_registry_params:
    force_creation: false # true or false
    # start and end of session frames to select for each participant. Keep empty or null for selecting all frames # -> only for pd_ds_paths_individual i.e individual paths
    session_frames: null # [0, 20] or keep null to incule all data 
    # selected_feature: "left_eye"  # only for pd_ds_paths_combinations i.e combined paths
    # selected_datasets: ["GFPGAN_x2", "SRResNet_x2"] # only for pd_ds_paths_combinations
    # ignored_datasets: [] # only for pd_ds_paths_combinations
  normalize: false
  # train_size: 0.7 # only for strategies other than cross_validation
  # val_size: 0.3   # only for strategies other than cross_validation
  dataset_registry: "PupilDiameter"
  dataset_registry_params:
    selected_targets: ["left_pupil"] # any from: ["left_pupil", "right_pupil", pupil_diameter (avg of left & right)]
    img_size: [32, 64] # eg:[32, 64], [64, 128]
    img_mode: "RGB" # any from: ["RGB", "YCbCr", "HSV", "LAB"] (modes which conserve 3 channels)
    augment: true # dataloader augment's by the given augmentation configurations
    augmentation_configs:
      rotations: [-5, 5] # randomly rotating images -5 or +5 degrees

dataloader_configs:
  batch_size: 128         # batch size will be set per gpu / cpu
  num_workers: 4
  pin_memory: true

# padded model
model_configs:
  registered_model_name: "ResNet18"
  # freeze_backbone: false
  # pretrained: false
  num_classes: 1 # should be same as the number of selected_targets

optimizer_configs:
  optimizer_name: "AdamW" # SGD, Adam, AdamW, any...
  lr: 0.001
  weight_decay: 0.01
  # momentum: 0.9

loss_function_configs:
  loss_function_name: "L1Loss" # L1Loss, MSELoss, HuberLoss, LogCoshLoss any...
  # reduction: "mean"
  # delta: 0.10

lr_scheduler_configs:
  scheduler_name: StepLR # MultiStepLR, ExponentialLR, any...
  step_size: 10 # [100, 200, 400, 600]
  gamma: 0.2 # 0.2, 0.5, 0.9, any...
  # warmup_configs:
  #   multiplier: 1
  #   total_epoch: 3

train_test_configs:
  eval_metrics: []
  eval_metrics_average: null
  max_steps: null         # max steps per epoch
  epochs: 50
  clip_grad_norm: null

# early_stopping_configs:
#   tracking_metric: "mae" # early stop based on the specified metric's prev and current score
#   patience: 100          # early stop based on the number of speficied epochs (i.e patience)

checkpointing_configs:
  resume: false
  run_id: null            # give mlflow existing run id when resume = true to resume logging (optional)
  checkpoint_path: null
  save_every_n_epoch: null
  tracking_metric: "mae" # track best model based on the specified metric scores

dist_params:
  backend: nccl
  
